{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace41c03-dfa3-4cb6-88bc-bcaa72cfdc85",
   "metadata": {},
   "source": [
    "# 使用RNN实现情感分类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1823275c-96a6-4c12-839c-623ac2662c6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 概述\n",
    "\n",
    "情感分类是自然语言处理中的经典任务，是典型的分类问题。本节使用MindSpore实现一个基于RNN网络的情感分类模型，实现如下的效果：\n",
    "\n",
    "```text\n",
    "[> : input, = : target, < : output]\n",
    "\n",
    "> This film is terrible\n",
    "= Negtive\n",
    "< Negtive\n",
    "\n",
    "> This film is great\n",
    "= Positive\n",
    "< Positive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4ac0d-886b-44a0-b794-ef7c4d867a3e",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db27766-c50a-4142-b107-8f96c877c7db",
   "metadata": {
    "tags": []
   },
   "source": [
    "本节使用情感分类的经典数据集[IMDB影评数据集](https://ai.stanford.edu/~amaas/data/sentiment/)，数据集包含Positive和Negative两类，下面为其样例：\n",
    "\n",
    "| Review  | Label  |\n",
    "|:---|:---:|\n",
    "| \"Quitting\" may be as much about exiting a pre-ordained identity as about drug withdrawal. As a rural guy coming to Beijing, class and success must have struck this young artist face on as an appeal to separate from his roots and far surpass his peasant parents' acting success. Troubles arise, however, when the new man is too new, when it demands too big a departure from family, history, nature, and personal identity. The ensuing splits, and confusion between the imaginary and the real and the dissonance between the ordinary and the heroic are the stuff of a gut check on the one hand or a complete escape from self on the other.  |  Negative |  \n",
    "| This movie is amazing because the fact that the real people portray themselves and their real life experience and do such a good job it's like they're almost living the past over again. Jia Hongsheng plays himself an actor who quit everything except music and drugs struggling with depression and searching for the meaning of life while being angry at everyone especially the people who care for him most.  | Positive  |\n",
    "\n",
    "此外，需要使用预训练词向量对自然语言单词进行编码，以获取文本的语义特征，本节选取[Glove](https://nlp.stanford.edu/projects/glove/)词向量作为Embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a74272-f34c-485c-8785-c220c9e6bc01",
   "metadata": {},
   "source": [
    "### 数据下载模块\n",
    "\n",
    "为了方便数据集和预训练词向量的下载，首先设计数据下载模块，实现可视化下载流程，并保存至指定路径。数据下载模块使用`requests`库进行http请求，并通过`tqdm`库对下载百分比进行可视化。此外针对下载安全性，使用IO的方式下载临时文件，而后保存至指定的路径并返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572e506f-169a-4a07-95a8-40d89fc39104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from typing import IO\n",
    "from pathlib import Path\n",
    "\n",
    "# 指定保存路径为 `home_path/.mindspore_examples`\n",
    "cache_dir = Path.home() / '.mindspore_examples'\n",
    "\n",
    "def http_get(url: str, temp_file:IO):\n",
    "    \"\"\"使用requests库下载数据，并使用tqdm库进行流程可视化\"\"\"\n",
    "    req = requests.get(url, stream=True)\n",
    "    content_length = req.headers.get('Content-Length')\n",
    "    total = int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(unit='B', total=total)\n",
    "    for chunk in req.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "def download(file_name:str, url: str):\n",
    "    \"\"\"下载数据并存为指定名称\"\"\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    cache_path = os.path.join(cache_dir, file_name)\n",
    "    cache_exist = os.path.exists(cache_path)\n",
    "    if not cache_exist:\n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            http_get(url, temp_file)\n",
    "            temp_file.flush()\n",
    "            temp_file.seek(0)\n",
    "            logging.info(f\"copying {temp_file.name} to cache at {cache_path}\")\n",
    "            with open(cache_path, 'wb') as cache_file:\n",
    "                shutil.copyfileobj(temp_file, cache_file)\n",
    "    return cache_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2340db-925a-4cc8-8d3d-545b13bda228",
   "metadata": {},
   "source": [
    "完成数据下载模块后，下载IMDB数据集进行测试(此处使用华为云的镜像下线用于提升下载速度)。下载过程及保存的路径如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0df701-e3b5-46e3-968d-9d44a2796eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.mindspore_examples/aclImdb_v1.tar.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_path = download('aclImdb_v1.tar.gz', 'https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz')\n",
    "imdb_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146320f-206d-4ce3-92b8-19c71d23e3d9",
   "metadata": {},
   "source": [
    "### 加载IMDB数据集\n",
    "\n",
    "下载好的IMDB数据集为`tar.gz`文件，我们使用Python的`tarfile`库对其进行读取，并将所有数据和标签分别进行存放。原始的IMDB数据集解压目录如下：\n",
    "\n",
    "```text\n",
    "    ├── aclImdb\n",
    "    │   ├── imdbEr.txt\n",
    "    │   ├── imdb.vocab\n",
    "    │   ├── README\n",
    "    │   ├── test\n",
    "    │   └── train \n",
    "    │         ├── neg\n",
    "    │         ├── pos\n",
    "    ...\n",
    "```\n",
    "\n",
    "数据集已分割为train和test两部分，且每部分包含neg和pos两个分类的文件夹，因此需分别train和test进行读取并处理数据和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f260663-5525-4831-ad5b-b24cdc2ee07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import string\n",
    "import tarfile\n",
    "\n",
    "class IMDBData():\n",
    "    \"\"\"IMDB数据集加载器\n",
    "    \n",
    "    加载IMDB数据集并处理为一个Python迭代对象。\n",
    "    \n",
    "    \"\"\"\n",
    "    label_map = {\n",
    "        \"pos\": 1,\n",
    "        \"neg\": 0\n",
    "    }\n",
    "    def __init__(self, path, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.path = path\n",
    "        self.docs, self.labels = [], []\n",
    "\n",
    "        self._load(\"pos\")\n",
    "        self._load(\"neg\")\n",
    "\n",
    "    def _load(self, label):\n",
    "        pattern = re.compile(r\"aclImdb/{}/{}/.*\\.txt$\".format(self.mode, label))\n",
    "        # 将数据加载至内存\n",
    "        with tarfile.open(self.path) as tarf:\n",
    "            tf = tarf.next()\n",
    "            while tf is not None:\n",
    "                if bool(pattern.match(tf.name)):\n",
    "                    # 对文本进行分词、去除标点和特殊字符、小写处理\n",
    "                    self.docs.append(str(tarf.extractfile(tf).read().rstrip(six.b(\"\\n\\r\"))\n",
    "                        .translate(None, six.b(string.punctuation)).lower(\n",
    "                        )).split())\n",
    "                    self.labels.append([self.label_map[label]])\n",
    "                tf = tarf.next()               \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.docs[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e5d172-f64d-4bb7-8cf2-d8e07d251c59",
   "metadata": {},
   "source": [
    "完成IMDB数据加载器后，加载训练数据集进行测试，输出数据集数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08908b9-4997-4c93-a1e1-d1573991d729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train = IMDBData(imdb_path, 'train')\n",
    "len(imdb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a05899-85f4-4e84-803a-e6afb3e784a7",
   "metadata": {},
   "source": [
    "将IMDB数据集加载至内存并构造为迭代对象后，可以使用`mindspore.dataset`提供的`Generatordataset`接口进行加载进行下一步的数据处理，下面封装一个函数将train和test分别使用`Generatordataset`进行加载，并指定数据集中文本和标签的`column_name`分别为`text`和`label`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf93849-0061-41e3-b49e-0f6475c84f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as dataset\n",
    "\n",
    "def load_imdb(imdb_path):\n",
    "    imdb_train = dataset.GeneratorDataset(IMDBData(imdb_path, \"train\"), column_names=[\"text\", \"label\"])\n",
    "    imdb_test = dataset.GeneratorDataset(IMDBData(imdb_path, \"test\"), column_names=[\"text\", \"label\"])\n",
    "    return imdb_train, imdb_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145600b6-a525-46dd-952c-8f83db5c7eae",
   "metadata": {},
   "source": [
    "加载IMDB数据集，可以看到imdb_train是一个GeneratorDataset对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2471d850-6770-4308-bcbe-40b5bbd919db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0x7febfafe8f10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train, imdb_test = load_imdb(imdb_path)\n",
    "imdb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee5b20-7547-4ca1-83fa-6ee083cc65a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 加载预训练词向量\n",
    "\n",
    "预训练词向量是对输入单词的数值化表示，通过`nn.Embedding`层，采用查表的方式，输入单词对应词表中的index，获得对应的表达向量。\n",
    "因此进行模型构造前，需要将Embedding层所需的词向量和词表进行构造。这里我们使用Glove(Global Vectors for Word Representation)这种经典的预训练词向量，\n",
    "其数据格式如下：\n",
    "\n",
    "| Word |  Vector |  \n",
    "|:---|:---:|\n",
    "| the | 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 ...|  \n",
    "| , | 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 ... | \n",
    "\n",
    "我们直接使用第一列的单词作为词表，使用`dataset.text.Vocab`将其按顺序加载；同时读取每一行的Vector并转为`numpy.array`，用于`nn.Embedding`加载权重使用。具体实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "519a863f-4053-4c20-93e2-b0acac829372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "def load_glove(glove_path):\n",
    "    glove_100d_path = os.path.join(cache_dir, 'glove.6B.100d.txt')\n",
    "    if not os.path.exists(glove_100d_path):\n",
    "        glove_zip = zipfile.ZipFile(glove_path)\n",
    "        glove_zip.extractall(cache_dir)\n",
    "\n",
    "    embeddings = []\n",
    "    tokens = []\n",
    "    with open(glove_100d_path, encoding='utf-8') as gf:\n",
    "        for glove in gf:\n",
    "            word, embedding = glove.split(maxsplit=1)\n",
    "            tokens.append(word)\n",
    "            embeddings.append(np.fromstring(embedding, dtype=np.float32, sep=' '))\n",
    "    # add embeddings for <unk> and <pad>\n",
    "    embeddings.append(np.random.rand(100))\n",
    "    embeddings.append(np.zeros((100,), np.float32))\n",
    "    \n",
    "    vocab = dataset.text.Vocab.from_list(tokens, special_tokens=[\"<unk>\", \"<pad>\"], special_first=False)\n",
    "    embeddings = np.array(embeddings).astype(np.float32)\n",
    "    return vocab, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa216e1-db92-4c74-868a-b0a1889810f3",
   "metadata": {},
   "source": [
    "由于数据集中可能存在词表没有覆盖的单词，因此需要加入`<unk>`标记符；同时由于输入长度的不一致，在打包为一个batch时需要将短的文本进行填充，因此需要加入`<pad>`标记符。完成后的词表长度为原词表长度+2。\n",
    "\n",
    "下面下载Glove词向量，并加载生成词表和词向量权重矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34ca0aee-7deb-45ea-8231-2be5f9c2e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_path = download('glove.6B.zip', 'https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/glove.6B.zip')\n",
    "vocab, embeddings = load_glove(glove_path)\n",
    "len(vocab.vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f199e78-e711-4801-87f0-a099289bdfc1",
   "metadata": {},
   "source": [
    "使用词表将`the`转换为index id，并查询词向量矩阵对应的词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b12e8554-496c-4f35-a0bc-67a3f60ff488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = vocab.tokens_to_ids('the')\n",
    "embedding = embeddings[idx]\n",
    "idx, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186eba95-4501-4427-9079-f5202f06e989",
   "metadata": {},
   "source": [
    "## 数据集预处理\n",
    "\n",
    "通过加载器加载的IMDB数据集进行了分词处理，但不满足构造训练数据的需要，因此要对其进行额外的预处理。其中包含的预处理如下：\n",
    "\n",
    "- 通过Vocab将所有的Token处理为index id。\n",
    "- 将文本序列统一长度，不足的使用`<pad>`补齐，超出的进行截断\n",
    "\n",
    "这里我们使用`mindspore.dataset`中提供的接口进行预处理操作。这里使用到的接口均为MindSpore的高性能数据引擎设计，每个接口对应操作视作数据流水线的一部分，详情请参考[MindSpore数据引擎]。(https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.6/design/data_engine.html)\n",
    "首先针对token到index id的查表操作，使用`text.Lookup`接口，将前文构造的词表加载，并指定`unknown_token`。其次为文本序列统一长度操作，使用`PadEnd`接口，此接口定义最大长度和补齐值(`pad_value`)，这里我们取最大长度为500，填充值对应词表中`<pad>`的index id.\n",
    "\n",
    "> 除了对数据集中`text`进行预处理外，由于后续模型训练的需要，要将`label`数据转为float32格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9451b7-e30b-4039-9955-40daac18b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "\n",
    "lookup_op = dataset.text.Lookup(vocab, unknown_token='<unk>')\n",
    "pad_op = dataset.transforms.c_transforms.PadEnd([500], pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "type_cast_op = dataset.transforms.c_transforms.TypeCast(mindspore.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e08575-85cf-4c50-8654-52bae0c3b413",
   "metadata": {},
   "source": [
    "完成预处理操作后，需将其加入到数据集处理流水线中，使用`map`接口对指定的column添加操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8cdc63-8460-4432-baa7-297acb17dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.map(operations=[lookup_op, pad_op],  input_columns=['text'])\n",
    "imdb_train = imdb_train.map(operations=[type_cast_op],  input_columns=['label'])\n",
    "\n",
    "imdb_test = imdb_test.map(operations=[lookup_op, pad_op],  input_columns=['text'])\n",
    "imdb_test = imdb_test.map(operations=[type_cast_op],  input_columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc8699-25b3-4d04-9f0c-f77c7a99703f",
   "metadata": {},
   "source": [
    "由于IMDB数据集本身不包含验证集，我们手动将其分割为训练和验证两部分，比例取0.7, 0.3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a62480-abb5-4a11-b83b-bef585e69066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:01:51.925.6 [mindspore/dataset/engine/datasets.py:1064] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "imdb_train, imdb_valid = imdb_train.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a81cf-2376-4840-9bda-84338897f8af",
   "metadata": {},
   "source": [
    "最后指定数据集的batch大小，通过`batch`接口指定，并设置是否丢弃无法被batch size整除的剩余数据。\n",
    "\n",
    "> 调用数据集的`map`,`split`,`batch`均为数据集处理流水线增加对应操作，返回值为新的Dataset类型。现在仅定义流水线操作，在执行时开始执行数据处理流水线，获取最终处理好的数据并送入模型进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9988cf-0dfb-424d-930a-1dc2f1ff9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.batch(64, drop_remainder=True)\n",
    "imdb_valid = imdb_valid.batch(64, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab4316-c35d-430d-bfaa-7af9b4f5b736",
   "metadata": {},
   "source": [
    "## 模型构建\n",
    "\n",
    "完成数据集的处理后，我们设计用于情感分类的模型结构。首先需要将输入文本(即序列化后的index id列表)通过查表转为向量化表示，此时需要使用`nn.Embedding`层加载Glove词向量；然后使用RNN循环神经网络做特征提取；最后将RNN连接至一个全连接层，即`nn.Dense`，将特征转化为与分类数量相同的size，用于后续进行模型优化训练。整体模型结构如下：\n",
    "\n",
    "```text\n",
    "nn.Embedding -> nn.RNN -> nn.Dense\n",
    "```\n",
    "\n",
    "这里我们使用能够一定程度规避RNN梯度消失问题的变种LSTM(Long short term memory)做特征提取层。下面对模型进行详解："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cdfd1-4fef-4394-a4ea-e5e7b509a9cc",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Embedding层又可称为EmbeddingLookup层，其作用是使用index id对权重矩阵对应id的向量进行查找，当输入为一个由index id组成的序列时，则查找并返回一个相同长度的矩阵，例如：\n",
    "\n",
    "```text\n",
    "embedding = nn.Embedding(1000, 100) # 词表大小(index的取值范围)为1000，表示向量的size为100\n",
    "input shape: (1, 16)                # 序列长度为16\n",
    "output shape：(1, 16, 100)\n",
    "```\n",
    "\n",
    "这里我们使用前文处理好的Glove词向量矩阵，设置`nn.Embedding`的`embedding_table`为预训练词向量矩阵。对应的`vocab_size`为词表大小400002，`embedding_size`为选用的`glove.6B.100d`向量大小，即100。\n",
    "\n",
    "#### RNN(循环神经网络)\n",
    "\n",
    "$$(h_t, c_t) = \\text{LSTM}(x_{0:t}, h_0, c_0)$$\n",
    "\n",
    "\n",
    "![](assets/sentiment2.png)\n",
    "\n",
    "##### Bidirectional RNN(双向RNN)\n",
    "\n",
    "![](assets/sentiment3.png)\n",
    "\n",
    "##### Multi-Layer RNN(多层RNN)\n",
    "\n",
    "![](assets/sentiment4.png)\n",
    "\n",
    "### Dense\n",
    "\n",
    "> 在Dense层后进行了`sigmoid`运算，将预测值归一至`[0,1]`区间，用于后续和`BCELoss`(BinaryCrossEntropyLoss)计算二分类交叉熵损失使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f1e26d-1b7c-4d50-947d-4fdda527b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "\n",
    "class RNN(nn.Cell):\n",
    "    def __init__(self, embeddings, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embeddings.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, embedding_table=Tensor(embeddings), padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(1 - dropout)\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        # inputs: (batch, seq_length)\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        # embedded: (batch, seq_length, embedding_dim)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        #outputs: (batch, seq_length, hidden_dim * num_directions)\n",
    "        #hidden: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        #cell: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        hidden = self.dropout(mnp.concatenate((hidden[-2,:,:], hidden[-1,:,:]), axis = 1))\n",
    "        output = self.fc(hidden)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e703dd4-aa69-4b16-a2c6-95219d7a95f5",
   "metadata": {},
   "source": [
    "### 损失函数与优化器\n",
    "\n",
    "完成模型主体构建后，首先根据指定的参数实例化网络；然后选择损失函数和优化器，并将其使用`nn.TrainOneStepCell`进行封装。针对本节情感分类问题的特性，即预测Positive或Negative的二分类问题，我们选择`nn.BCELoss`(二分类交叉熵损失函数)，这里也可以选择`nn.BCEWithLogitsLoss`, 其包含`sigmoid`运算，即：\n",
    "\n",
    "```\n",
    "BCEWithLogitsLoss = Sigmoid + BCELoss\n",
    "```\n",
    "这里使用`BECLoss`需要设置`reduction`参数为均值。而后使用`nn.WithLossCell`将其与实例化网络对象进行关联。\n",
    "\n",
    "选择合适的损失函数后，选择`Adam`优化器，并将二者传入`TrainOneStepCell`中。\n",
    "\n",
    "> MindSpore的设计理念为整图计算与优化，因此将Loss function和Optimizer均视为计算图的一部分，所以构造`TrainOneStepCell`作为Wrapper使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591b1301-e9e3-44eb-a88b-09d2e807f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "pad_idx = vocab.tokens_to_ids('<pad>')\n",
    "\n",
    "net = RNN(embeddings, hidden_size, output_size, num_layers, bidirectional, dropout, pad_idx)\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "net_with_loss = nn.WithLossCell(net, loss)\n",
    "optimizer = nn.Adam(net.trainable_params())\n",
    "train_one_step = nn.TrainOneStepCell(net_with_loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ee0e4-2381-4c4c-b493-60c40396165d",
   "metadata": {},
   "source": [
    "### 训练逻辑\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5479830-8370-484d-a6ae-2a5985ab43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataset, epoch=0):\n",
    "    model.set_train()\n",
    "    total = train_dataset.get_dataset_size()\n",
    "    loss_total = 0\n",
    "    step_total = 0\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in train_dataset.create_tuple_iterator():\n",
    "            loss = model(*i)\n",
    "            loss_total += loss.asnumpy()\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=loss_total/step_total)\n",
    "            t.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffc4b0-2779-46a8-a952-7d4f404943f6",
   "metadata": {},
   "source": [
    "### 评估指标和逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "028a0b84-deee-4d1b-a1ad-a6ad515fd22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = np.around(preds)\n",
    "    correct = (rounded_preds == y).astype(np.float32) #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78bfafbc-3a10-4aab-b9bb-3189f84bc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, criterion, epoch=0):\n",
    "    total = test_dataset.get_dataset_size()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    step_total = 0\n",
    "    model.set_train(False)\n",
    "\n",
    "    iterator = test_dataset.create_tuple_iterator()\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in test_dataset.create_tuple_iterator():\n",
    "            predictions = model(i[0])\n",
    "            loss = criterion(predictions, i[1])\n",
    "            epoch_loss += loss.asnumpy()\n",
    "\n",
    "            acc = binary_accuracy(predictions.asnumpy(), i[1].asnumpy())\n",
    "            epoch_acc += acc\n",
    "\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=epoch_loss/step_total, acc=epoch_acc/step_total)\n",
    "            t.update(1)\n",
    "\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a6e44-3401-4b5d-b169-6f33584670a0",
   "metadata": {},
   "source": [
    "## 模型训练与保存\n",
    "\n",
    "> MindSpore默认采用静态图模式(即Define and Run)进行训练，第一个step会进行计算图编译操作，较为耗时，但整体训练效率更高，若需要进行单步调试或使用动态图模式，可使用如下代码进行设置：\n",
    "> ```python\n",
    "> from mindspore import context\n",
    "> context.set_context(mode=context.PYNATIVE_MODE)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851ec476-ca3a-439c-95db-55312dbda7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████████| 273/273 [00:50<00:00,  5.42it/s, loss=0.662]\n",
      "Epoch 0: 100%|███████████████████████████████████████████████| 117/117 [00:44<00:00,  2.65it/s, acc=0.646, loss=0.626]\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████████| 273/273 [00:44<00:00,  6.11it/s, loss=0.608]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████| 117/117 [00:43<00:00,  2.71it/s, acc=0.712, loss=0.547]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████| 273/273 [00:44<00:00,  6.10it/s, loss=0.547]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████| 117/117 [00:43<00:00,  2.72it/s, acc=0.822, loss=0.423]\n",
      "Epoch 3: 100%|██████████████████████████████████████████████████████████| 273/273 [00:44<00:00,  6.10it/s, loss=0.428]\n",
      "Epoch 3: 100%|███████████████████████████████████████████████| 117/117 [00:43<00:00,  2.71it/s, acc=0.893, loss=0.303]\n",
      "Epoch 4: 100%|██████████████████████████████████████████████████████████| 273/273 [00:44<00:00,  6.14it/s, loss=0.304]\n",
      "Epoch 4: 100%|███████████████████████████████████████████████| 117/117 [00:43<00:00,  2.71it/s, acc=0.886, loss=0.286]\n"
     ]
    }
   ],
   "source": [
    "from mindspore import save_checkpoint\n",
    "\n",
    "num_epochs = 5\n",
    "best_valid_loss = float('inf')\n",
    "ckpt_file_name = os.path.join(cache_dir, 'sentiment-analysis.ckpt')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(train_one_step, imdb_train, epoch)\n",
    "    valid_loss = evaluate(net, imdb_valid, loss, epoch)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        save_checkpoint(net, ckpt_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc7b0d-9055-4cab-9b6d-46bc0b06f60a",
   "metadata": {},
   "source": [
    "## 模型加载与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d477ec12-04ff-488b-8d31-ae82e5e9be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.483.329 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(400002, 100), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.489.198 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 100), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.490.350 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 100), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.491.731 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 512), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.493.170 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 512), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.494.323 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 256), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.495.388 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 256), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.496.448 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 256), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.497.496 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024, 256), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.498.331 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.499.128 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.500.294 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.501.353 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.502.336 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.503.295 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.504.229 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.505.199 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1024,), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.506.232 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1, 512), dtype=Float32, requires_grad=True)'.\n",
      "[WARNING] ME(19512:140655497611072,MainProcess):2022-02-24-03:09:19.509.801 [mindspore/common/parameter.py:338] The parameter definition is deprecated.\n",
      "Please set a unique name for the parameter 'Parameter (name=Parameter, shape=(1,), dtype=Float32, requires_grad=True)'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "param_dict = load_checkpoint(ckpt_file_name)\n",
    "load_param_into_net(net, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3969ddd-ef79-433b-8f3a-dafaaf0a0540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████| 391/391 [00:33<00:00, 11.82it/s, acc=0.831, loss=0.389]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38909361658193875"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_test = imdb_test.batch(64)\n",
    "evaluate(net, imdb_test, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5be370-edc3-4453-802a-f5eea34cd77f",
   "metadata": {},
   "source": [
    "## 自定义输入测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b35902c6-84a3-4879-8fde-2d6f322f49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map = {\n",
    "    1: \"Positive\",\n",
    "    0: \"Negative\"\n",
    "}\n",
    "\n",
    "def predict_sentiment(model, vocab, sentence):\n",
    "    model.set_train(False)\n",
    "    tokenized = sentence.lower().split()\n",
    "    indexed = vocab.tokens_to_ids(tokenized)\n",
    "    tensor = mindspore.Tensor(indexed, mindspore.int32)\n",
    "    tensor = tensor.expand_dims(0)\n",
    "    prediction = model(tensor)\n",
    "    return score_map[int(np.round(prediction.asnumpy()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f6ea007-96a9-4b7c-9f5c-59329c0c7de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6d9108e-8151-4ccc-a433-b33f437f1c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, \"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bb944-4036-4746-8d39-78dd8d830966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
