{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace41c03-dfa3-4cb6-88bc-bcaa72cfdc85",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4ac0d-886b-44a0-b794-ef7c4d867a3e",
   "metadata": {},
   "source": [
    "## 基础下载模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572e506f-169a-4a07-95a8-40d89fc39104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "from typing import IO\n",
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path.home() / '.mindspore_examples'\n",
    "\n",
    "def http_get(url: str, temp_file:IO):\n",
    "    req = requests.get(url, stream=True)\n",
    "    content_length = req.headers.get('Content-Length')\n",
    "    total = int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(unit='B', total=total)\n",
    "    for chunk in req.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "def download(file_name:str, url: str):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    cache_path = os.path.join(cache_dir, file_name)\n",
    "    cache_exist = os.path.exists(cache_path)\n",
    "    if not cache_exist:\n",
    "        with tempfile.NamedTemporaryFile() as temp_file:\n",
    "            http_get(url, temp_file)\n",
    "            temp_file.flush()\n",
    "            temp_file.seek(0)\n",
    "            logging.info(f\"copying {temp_file.name} to cache at {cache_path}\")\n",
    "            with open(cache_path, 'wb') as cache_file:\n",
    "                shutil.copyfileobj(temp_file, cache_file)\n",
    "    return cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0df701-e3b5-46e3-968d-9d44a2796eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = download('aclImdb_v1.tar.gz', 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146320f-206d-4ce3-92b8-19c71d23e3d9",
   "metadata": {},
   "source": [
    "## 加载IMDB数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf93849-0061-41e3-b49e-0f6475c84f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import mindspore.dataset as dataset\n",
    "\n",
    "def load_imdb(imdb_path):\n",
    "    imdb_extract_path = os.path.join(cache_dir, 'aclImdb')\n",
    "    if not os.path.exists(imdb_extract_path):\n",
    "        tar = tarfile.open(imdb_path)\n",
    "        tar.extractall(path=cache_dir)\n",
    "    imdb_train = dataset.IMDBDataset(imdb_extract_path, usage=\"train\", shuffle=True)\n",
    "    imdb_test = dataset.IMDBDataset(imdb_extract_path, usage=\"test\", shuffle=False)\n",
    "    return imdb_train, imdb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2471d850-6770-4308-bcbe-40b5bbd919db",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, imdb_test = load_imdb(imdb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee5b20-7547-4ca1-83fa-6ee083cc65a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 下载预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ca0aee-7deb-45ea-8231-2be5f9c2e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = download('glove.6B.zip', 'https://nlp.stanford.edu/data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519a863f-4053-4c20-93e2-b0acac829372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "def load_glove(glove_path):\n",
    "    glove_100d_path = os.path.join(cache_dir, 'glove.6B.100d.txt')\n",
    "    if not os.path.exists(glove_100d_path):\n",
    "        glove_zip = zipfile.ZipFile(glove_path)\n",
    "        glove_zip.extractall(cache_dir)\n",
    "\n",
    "    embeddings = []\n",
    "    tokens = []\n",
    "    with open(glove_100d_path, encoding='utf-8') as gf:\n",
    "        for glove in gf:\n",
    "            word, embedding = glove.split(maxsplit=1)\n",
    "            tokens.append(word)\n",
    "            embeddings.append(np.fromstring(embedding, dtype=np.float32, sep=' '))\n",
    "    # add embeddings for <unk> and <pad>\n",
    "    embeddings.append(np.random.randn(100))\n",
    "    embeddings.append(np.zeros((100,), np.float32))\n",
    "    \n",
    "    vocab = dataset.text.Vocab.from_list(tokens, special_tokens=[\"<unk>\", \"<pad>\"], special_first=False)\n",
    "    embeddings = np.array(embeddings).astype(np.float32)\n",
    "    return vocab, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8337d52c-aa9c-4316-919e-a901899a78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, embeddings = load_glove(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12e8554-496c-4f35-a0bc-67a3f60ff488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = vocab.tokens_to_ids('the')\n",
    "embedding = embeddings[idx]\n",
    "idx, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186eba95-4501-4427-9079-f5202f06e989",
   "metadata": {},
   "source": [
    "## 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a317d1ac-bba2-4ee0-815a-7630b072c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9451b7-e30b-4039-9955-40daac18b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import six\n",
    "import spacy\n",
    "import mindspore\n",
    "\n",
    "spacy_tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(line):\n",
    "    line = line.replace(\"<br />\", ' ').lower()\n",
    "    tokens = spacy_tokenizer(line)\n",
    "    return [i.text for i in tokens]\n",
    "\n",
    "def expand_dim(label):\n",
    "    return np.expand_dims(np.array(label), -1)\n",
    "\n",
    "tokenizer_op = dataset.text.PythonTokenizer(tokenizer)\n",
    "lookup_op = dataset.text.Lookup(vocab, unknown_token='<unk>')\n",
    "pad_op = dataset.transforms.c_transforms.PadEnd([500], pad_value=vocab.tokens_to_ids('<pad>'))\n",
    "type_cast_op = dataset.transforms.c_transforms.TypeCast(mindspore.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8cdc63-8460-4432-baa7-297acb17dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.map(operations=[tokenizer_op, lookup_op, pad_op],  input_columns=['text'])\n",
    "imdb_train = imdb_train.map(operations=[type_cast_op, expand_dim],  input_columns=['label'])\n",
    "\n",
    "imdb_test = imdb_test.map(operations=[tokenizer_op, lookup_op, pad_op],  input_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be28663-85b3-4464-a8d0-dbe2ef6001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f1e26d-1b7c-4d50-947d-4fdda527b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Cell):\n",
    "    def __init__(self, embeddings, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        vocab_size, embedding_dim = embeddings.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, embedding_table=Tensor(embeddings), padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(1 - dropout)\n",
    "        self.sigmoid = ops.Sigmoid()\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        # inputs: (batch, seq_length)\n",
    "        embedded = self.dropout(self.embedding(inputs))\n",
    "        # embedded: (batch, seq_length, embedding_dim)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        #outputs: (batch, seq_length, hidden_dim * num_directions)\n",
    "        #hidden: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        #cell: (num_layers * num_directions, batch_size, hidden_dim)\n",
    "        hidden = self.dropout(mnp.concatenate((hidden[-2,:,:], hidden[-1,:,:]), axis = 1))\n",
    "        output = self.fc(hidden)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "591b1301-e9e3-44eb-a88b-09d2e807f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "output_size = 1\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "pad_idx = vocab.tokens_to_ids('<pad>')\n",
    "\n",
    "model = RNN(embeddings, hidden_size, output_size, num_layers, bidirectional, dropout, pad_idx)\n",
    "loss = nn.BCELoss(reduction='mean')\n",
    "model_with_loss = nn.WithLossCell(model, loss)\n",
    "optimizer = nn.Adam(model.trainable_params())\n",
    "train_one_step = nn.TrainOneStepCell(model_with_loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9988cf-0dfb-424d-930a-1dc2f1ff9176",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc7920b-2ace-41eb-8fb2-b85fcbe3676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_step.compile(*next(imdb_train.create_tuple_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5479830-8370-484d-a6ae-2a5985ab43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataset, epoch=0):\n",
    "    total = train_dataset.get_dataset_size()\n",
    "    loss_total = 0\n",
    "    step_total = 0\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in train_dataset.create_tuple_iterator():\n",
    "            loss = train_one_step(i[0], i[1])\n",
    "            loss_total += loss.asnumpy()\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=loss_total/step_total)\n",
    "            t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1b117c-fe49-47dc-a5bd-f2455ba7b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  33%|██████████████████████████████████████████████████████▉                                                                                                                 | 128/391 [12:17<23:09,  5.28s/it, loss=0.664]"
     ]
    }
   ],
   "source": [
    "train_one_epoch(train_one_step, imdb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c76d0-3958-45d8-9c23-b455ab57963d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
