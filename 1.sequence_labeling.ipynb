{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66014f9c-60b8-4cb4-b5c0-3f387aaf01af",
   "metadata": {},
   "source": [
    "# LSTM+CRF实现序列标注"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77717a-465c-44cd-8d4d-37c6f8f132b8",
   "metadata": {},
   "source": [
    "$$\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\end{align}$$\n",
    "\n",
    "$$\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}$$\n",
    "\n",
    "$$\\begin{align}= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "120a39c7-c89d-4cd3-8a75-da27d2853f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as mnp\n",
    "from mindspore import Parameter\n",
    "from mindspore.common.initializer import initializer, Uniform\n",
    "\n",
    "def sequence_mask(seq_length, max_length, batch_first=False):\n",
    "    \"\"\"generate mask matrix by seq_length\"\"\"\n",
    "    range_vector = mnp.arange(0, max_length, 1, seq_length.dtype)\n",
    "    result = range_vector < seq_length.view(seq_length.shape + (1,))\n",
    "    if batch_first:\n",
    "        return result.astype(mindspore.int64)\n",
    "    return result.astype(mindspore.int64).swapaxes(0, 1)\n",
    "\n",
    "class CRF(nn.Cell):\n",
    "    def __init__(self, num_tags: int, batch_first: bool = False, reduction: str = 'sum') -> None:\n",
    "        if num_tags <= 0:\n",
    "            raise ValueError(f'invalid number of tags: {num_tags}')\n",
    "        super().__init__()\n",
    "        if reduction not in ('none', 'sum', 'mean', 'token_mean'):\n",
    "            raise ValueError(f'invalid reduction: {reduction}')\n",
    "        self.num_tags = num_tags\n",
    "        self.batch_first = batch_first\n",
    "        self.reduction = reduction\n",
    "        self.start_transitions = Parameter(initializer(Uniform(0.1), (num_tags,)), name='start_transitions')\n",
    "        self.end_transitions = Parameter(initializer(Uniform(0.1), (num_tags,)), name='end_transitions')\n",
    "        self.transitions = Parameter(initializer(Uniform(0.1), (num_tags, num_tags)), name='transitions')\n",
    "\n",
    "    def construct(self, emissions, tags=None, seq_length=None):\n",
    "        if tags is None:\n",
    "            return self._decode(emissions, seq_length)\n",
    "        return self._forward(emissions, tags, seq_length)\n",
    "\n",
    "    def _forward(self, emissions, tags=None, seq_length=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, max_length = tags.shape\n",
    "            emissions = emissions.swapaxes(0, 1)\n",
    "            tags = tags.swapaxes(0, 1)\n",
    "        else:\n",
    "            max_length, batch_size = tags.shape\n",
    "\n",
    "        if seq_length is None:\n",
    "            seq_length = mnp.full((batch_size,), max_length, mindspore.int64)\n",
    "        \n",
    "        mask = sequence_mask(seq_length, max_length)\n",
    "\n",
    "        # shape: (batch_size,)\n",
    "        numerator = self._compute_score(emissions, tags, seq_length-1, mask)\n",
    "        # shape: (batch_size,)\n",
    "        denominator = self._compute_normalizer(emissions, mask)\n",
    "        # shape: (batch_size,)\n",
    "        llh = denominator - numerator\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return llh\n",
    "        elif self.reduction == 'sum':\n",
    "            return llh.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            return llh.mean()\n",
    "        return llh.sum() / mask.astype(emissions.dtype).sum()\n",
    "\n",
    "    def _decode(self, emissions, seq_length=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, max_length = emissions.shape[:2]\n",
    "            emissions = emissions.swapaxes(0, 1)\n",
    "        else:\n",
    "            batch_size, max_length = emissions.shape[:2]\n",
    "\n",
    "        if seq_length is None:\n",
    "            seq_length = mnp.full((batch_size,), max_length, mindspore.int64)\n",
    "        \n",
    "        mask = sequence_mask(seq_length, max_length)\n",
    "\n",
    "        return self._viterbi_decode(emissions, mask)\n",
    "\n",
    "    def _compute_score(self, emissions, tags, seq_ends, mask):\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # tags: (seq_length, batch_size)\n",
    "        # mask: (seq_length, batch_size)\n",
    "\n",
    "        seq_length, batch_size = tags.shape\n",
    "        mask = mask.astype(emissions.dtype)\n",
    "\n",
    "        # Start transition score and first emission\n",
    "        # shape: (batch_size,)\n",
    "        score = self.start_transitions[tags[0]]\n",
    "        score += emissions[0, mnp.arange(batch_size), tags[0]]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n",
    "\n",
    "            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n",
    "            # shape: (batch_size,)\n",
    "            score += emissions[i, mnp.arange(batch_size), tags[i]] * mask[i]\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size,)\n",
    "        last_tags = tags[seq_ends, mnp.arange(batch_size)]\n",
    "        # shape: (batch_size,)\n",
    "        score += self.end_transitions[last_tags]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _compute_normalizer(self, emissions, mask):\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "\n",
    "        seq_length = emissions.shape[0]\n",
    "\n",
    "        # Start transition score and first emission; score has size of\n",
    "        # (batch_size, num_tags) where for each batch, the j-th column stores\n",
    "        # the score that the first timestep has tag j\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.expand_dims(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emissions = emissions[i].expand_dims(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the sum of scores of all\n",
    "            # possible tag sequences so far that end with transitioning from tag i to tag j\n",
    "            # and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emissions\n",
    "\n",
    "            # Sum over all possible current tags, but we're in score space, so a sum\n",
    "            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n",
    "            # all possible tag sequences so far, that end in tag i\n",
    "            # shape: (batch_size, num_tags)\n",
    "            next_score = mnp.log(mnp.sum(mnp.exp(next_score), axis=1))\n",
    "\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = mnp.where(mask[i].expand_dims(1), next_score, score)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        # Sum (log-sum-exp) over all possible tags\n",
    "        # shape: (batch_size,)\n",
    "        return mnp.log(mnp.sum(mnp.exp(score), axis=1))\n",
    "\n",
    "    def _viterbi_decode(self, emissions, mask):\n",
    "        # emissions: (seq_length, batch_size, num_tags)\n",
    "        # mask: (seq_length, batch_size)\n",
    "\n",
    "        seq_length = mask.shape[0]\n",
    "\n",
    "        # Start transition and first emission\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score = self.start_transitions + emissions[0]\n",
    "        history = ()\n",
    "\n",
    "        # score is a tensor of size (batch_size, num_tags) where for every batch,\n",
    "        # value at column j stores the score of the best tag sequence so far that ends\n",
    "        # with tag j\n",
    "        # history saves where the best tags candidate transitioned from; this is used\n",
    "        # when we trace back the best tag sequence\n",
    "\n",
    "        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n",
    "        # for every possible next tag\n",
    "        for i in range(1, seq_length):\n",
    "            # Broadcast viterbi score for every possible next tag\n",
    "            # shape: (batch_size, num_tags, 1)\n",
    "            broadcast_score = score.expand_dims(2)\n",
    "\n",
    "            # Broadcast emission score for every possible current tag\n",
    "            # shape: (batch_size, 1, num_tags)\n",
    "            broadcast_emission = emissions[i].expand_dims(1)\n",
    "\n",
    "            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n",
    "            # for each sample, entry at row i and column j stores the score of the best\n",
    "            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n",
    "            # shape: (batch_size, num_tags, num_tags)\n",
    "            next_score = broadcast_score + self.transitions + broadcast_emission\n",
    "\n",
    "            # Find the maximum score over all possible current tag\n",
    "            # shape: (batch_size, num_tags)\n",
    "            indices = next_score.argmax(axis=1)\n",
    "            next_score = next_score.max(axis=1)\n",
    "            # Set score to the next score if this timestep is valid (mask == 1)\n",
    "            # and save the index that produces the next score\n",
    "            # shape: (batch_size, num_tags)\n",
    "            score = mnp.where(mask[i].expand_dims(1), next_score, score)\n",
    "            history += (indices,)\n",
    "\n",
    "        # End transition score\n",
    "        # shape: (batch_size, num_tags)\n",
    "        score += self.end_transitions\n",
    "\n",
    "        return score, history\n",
    "\n",
    "    def post_decode(self, score, history, seq_length):\n",
    "        # Now, compute the best path for each sample\n",
    "        batch_size = seq_length.shape[0]\n",
    "        seq_ends = seq_length - 1\n",
    "        # shape: (batch_size,)\n",
    "        best_tags_list = []\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            # Find the tag which maximizes the score at the last timestep; this is our best tag\n",
    "            # for the last timestep\n",
    "            best_last_tag = score[idx].argmax(axis=0)\n",
    "            best_tags = [int(best_last_tag.asnumpy())]\n",
    "            # We trace back where the best last tag comes from, append that to our best tag\n",
    "            # sequence, and trace it back again, and so on\n",
    "            for hist in reversed(history[:seq_ends[idx]]):\n",
    "                best_last_tag = hist[idx][best_tags[-1]]\n",
    "                best_tags.append(int(best_last_tag.asnumpy()))\n",
    "\n",
    "            # Reverse the order because we start from the last timestep\n",
    "            best_tags.reverse()\n",
    "            best_tags_list.append(best_tags)\n",
    "\n",
    "        return best_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07555e3-f2a2-4c25-beff-5a78491ab2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Cell):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_tags, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Dense(hidden_dim, num_tags)\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "    def construct(self, inputs, seq_length, tags=None):\n",
    "        embeds = self.embedding(inputs)\n",
    "        outputs, _ = self.lstm(embeds, seq_length=seq_length)\n",
    "        feats = self.hidden2tag(outputs)\n",
    "        \n",
    "        crf_outs = self.crf(feats, tags, seq_length)\n",
    "        return crf_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa53f535-34bc-49a3-b769-e85d3a184cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 5\n",
    "hidden_dim = 4\n",
    "\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "\n",
    "word_to_idx = {}\n",
    "word_to_idx['<pad>'] = 0\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "tag_to_idx = {\"B\": 0, \"I\": 1, \"O\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c255358-938b-4e88-a810-2fe4e616a044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d3f2bb-0ea7-457c-8a74-a8e0ea78a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_idx), embedding_dim, hidden_dim, len(tag_to_idx))\n",
    "optimizer = nn.SGD(model.trainable_params(), learning_rate=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866a32ed-4294-4816-bcac-c56f0a491dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_step = nn.TrainOneStepCell(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7899b6e6-95a4-4ffe-ba49-b27cebe8b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seqs, word_to_idx, tag_to_idx):\n",
    "    seq_outputs, label_outputs, seq_length = [], [], []\n",
    "    max_len = max([len(i[0]) for i in seqs])\n",
    "\n",
    "    for seq, tag in seqs:\n",
    "        seq_length.append(len(seq))\n",
    "        idxs = [word_to_idx[w] for w in seq]\n",
    "        labels = [tag_to_idx[t] for t in tag]\n",
    "        idxs.extend([word_to_idx['<pad>'] for i in range(max_len - len(seq))])\n",
    "        labels.extend([tag_to_idx['O'] for i in range(max_len - len(seq))])\n",
    "        seq_outputs.append(idxs)\n",
    "        label_outputs.append(labels)\n",
    "\n",
    "    return mindspore.Tensor(seq_outputs, mindspore.int64), \\\n",
    "            mindspore.Tensor(label_outputs, mindspore.int64), \\\n",
    "            mindspore.Tensor(seq_length, mindspore.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e984dcb-6f89-4520-940f-620cdb997529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 11), (2, 11), (2,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label, seq_length = prepare_sequence(training_data, word_to_idx, tag_to_idx)\n",
    "data.shape, label.shape, seq_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46595499-c50e-48ea-82ff-e006b00aba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                            | 0/500 [00:00<?, ?it/s][WARNING] DEBUG(20523,7f26d8232740,python):2022-03-04-11:27:11.202.376 [mindspore/ccsrc/debug/debugger/debugger.cc:95] Debugger] Not enabling debugger. Debugger does not support CPU.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 21.76it/s, loss=[0.81230736 0.29350662]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "steps = 500\n",
    "with tqdm(total=steps) as t:\n",
    "    for i in range(steps):\n",
    "        loss = train_one_step(data, seq_length, label)\n",
    "        t.set_postfix(loss=loss)\n",
    "        t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65304086-de03-4edc-b3a2-3ca5e43c4795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 3], dtype=Float32, value=\n",
       "[[ 2.58174553e+01,  2.04221268e+01,  2.78829422e+01],\n",
       " [ 1.92055836e+01,  1.57861795e+01,  1.70711060e+01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, histroy = model(data, seq_length)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5ff78a-099c-41f8-ac1e-8f9fbfab7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.crf.post_decode(score, histroy, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df505983-9c1f-4e09-8a67-1cd78ce69fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2], [0, 1, 2, 2, 2, 2, 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
